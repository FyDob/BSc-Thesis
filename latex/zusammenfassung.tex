\section*{Zusammenfassung}
Die Fähigkeit von drei bekannten RNN Architekturen - SRNN, LSTM und GRU - die unterliegende Struktur der Dyck(2)-Grammatik zu lernen wurde untersucht. Die Faktoren von Modellkomplexität und Korpuskomposition wurden berücksichtigt, indem zu jeder Architektur je $27$ Modelle trainiert worden sind - je ein Modell mit $n$ versteckten Einheiten ($n = \lbrace 2^{1}, 2^{2}, \dots, 2^{9} \rbrace$) wurde auf einem von drei Korpora trainiert. Insgesamt wurden $81$ Modelle trainiert und getestet. Die drei Korpora waren ein Baseline Korpus, zu dem die anderen verglichen wurden. Zusätzlich wurde ein Korpus kreiert, in dem sich Abhängigkeiten zwischen einzelnen Buchstaben häufig lange erstrecken (High LRD), sowie ein Korpus wo diese Abhängigkeiten häufig kurz sind (Low LRD). Die Modelle wurden auf zwei verschiedenen Klassifikations-Experimenten evaluiert, welche darauf ausgelegt waren, das Modellverhalten bei langen Abhängigkeiten und tiefer Rekursion zu untersuchen. Die Ergebnisse wurden in Form von Genauigkeit, Precision, Recall und F1 Score angegeben. Zusätzlich wurde die Verteilung von falsch als korrekt klassifizierten Wörtern betrachtet. Die Komplexität des Trainingkorpus' hat sich als großer Einfluss auf die Lernfähigkeit von Modellen erwiesen: Modelle, die auf dem Low LRD Korpus trainiert worden sind, waren erfolgreicher als die Baseline Modelle, während High LRD Modelle weniger erfolgreich waren.