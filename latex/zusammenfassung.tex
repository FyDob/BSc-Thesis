\section*{Zusammenfassung}
%Die Fähigkeit von drei bekannten RNN Architekturen - SRNN, LSTM und GRU - die unterliegende Struktur der Dyck(2)-Grammatik zu lernen wurde untersucht. Die Faktoren von Modellkomplexität und Korpuskomposition wurden berücksichtigt, indem zu jeder Architektur je $27$ Modelle trainiert worden sind - je ein Modell mit $n$ versteckten Einheiten ($n = \lbrace 2^{1}, 2^{2}, \dots, 2^{9} \rbrace$) wurde auf einem von drei Korpora trainiert. Insgesamt wurden $81$ Modelle trainiert und getestet. Die drei Korpora waren ein Baseline Korpus, zu dem die anderen verglichen wurden. Zusätzlich wurde ein Korpus kreiert, in dem sich Abhängigkeiten zwischen einzelnen Buchstaben häufig lange erstrecken (High LRD), sowie ein Korpus wo diese Abhängigkeiten häufig kurz sind (Low LRD). Die Modelle wurden auf zwei verschiedenen Vorhersage-Experimenten evaluiert, welche darauf ausgelegt waren, das Modellverhalten bei langen Abhängigkeiten und tiefer Rekursion zu untersuchen. Die Ergebnisse wurden in Form von Genauigkeit-per-Buchstabenposition und Konfusionsmatrizen gemeldet. Insgesamt sind die Modelle in der Lage, große Teile von Dyck(2) zu modellieren, scheitern jedoch an Rekursionstiefe. Desweiteren zeigen die Ergebnisse einen starken Einfluss von Korpuskomposition: Alle auf High LRD trainierten Modelle scheitern daran, Dyck(2) zu lernen, während alle Low LRD trainierten Modelle Erfolg haben.