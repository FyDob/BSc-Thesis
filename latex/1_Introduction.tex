\section{Introduction}\label{ch:introduction}
In the 2010s, seemingly every major technology company developed its own "personal assistant" system, a program that allows the end-user to interact with the company's services more intuitively by interpreting spoken natural language commands. Apple's Siri, Amazon's Alexa and Google's succinctly named Assistant have been irrevocably ingrained in day-to-day life. While the ethical and data security concerns raised by this development are still a point of contention, it is clear that Natural Language Processing (NLP) applications have boomed from a niche field to a rapidly growing multi-million dollar industry\footnote{https://www.tractica.com/newsroom/press-releases/natural-language-processing-market-to-reach-22-3-billion-by-2025/}.
Despite state-of-the-art performance on NLP tasks such as machine translation, text classification, sentiment analysis and speech recognition having made leaps and bounds in the past decade, these systems are still far from acquiring a perfect understanding of natural language. Recently, many new Recurrent Neural Network (RNN) model ideas have been experimented with, like the Clockwork RNN (\cite{Koutnik2014}) or the Recurrent Unit with a Stack-like State (RUSS) (\cite{Bernardy2018}), often designed to excel at a specific task. To showcase the new model's superiority, its performance on a task is usually compared to that of a more well-known architecture, such as the Simple RNN (SRNN), the Long Short Term Memory (LSTM) or the Gated Recurrent Unit (GRU).

What is missing from the current state of literature is, however, a robust comparison of these three architectures on a task that adequately showcases their respective ability to perform well on natural language data. I seek to fill that gap with my work by trying to answer the following questions:
\begin{enumerate}
\item Can an SRNN, LSTM or GRU architecture learn the Dyck language with two pairs of brackets ($D_{2}$)?
\item If they cannot, what poses the highest difficulty in doing so?
\item What influence, if any, does corpus construction have on model performance, specifically generalizability?
\end{enumerate}
The following work consists of five main parts, each of which will be briefly summarized hereunder:

In Chapter \ref{ch:theoreticalBackground}, I introduce the core concepts relevant for this thesis: formal languages, the complexity of Natural Language, Dyck languages, three neural network architectures and an overview of related works to contextualize my work within the current state of research. I describe model design and training, corpus construction and the two experiments I conduct in this work in Chapter \ref{ch:experimentSetup}. I report the results for each of the experiments in Chapter \ref{ch:results} and discuss them in detail in Chapter \ref{ch:discussion}. Finally, I seek to answer the research questions posed above with my experimental results in Chapter \ref{ch:conclusion} and suggest further avenues of research on this topic.