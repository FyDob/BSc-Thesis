\section*{Summary}
The capability of three major recurrent neural network (RNN) architectures - SRNN, LSTM and GRU - to learn the underlying structure of the Dyck(2) grammar has been investigated. To assess the influence of such factors as model complexity and training corpus composition, each architecture was instantiated in $27$ models, with a model containing $n$ hidden units ($n = \lbrace 2^{1}, 2^{2}, \dots, 2^{9} \rbrace$) each being trained on a character level on one of three corpora. In total, $81$ models were trained and tested. The corpora consisted of a Baseline corpus to compare to, as well as a corpus containing an increased frequency for long-range dependencies (High LRD) and one with a decreased frequency thereof (Low LRD). The models were evaluated on two classification experiments, designed to assess handling of long-range dependencies and increasing nesting levels. The results were reported in terms of accuracy, precision, recall and F1 score, as well as a closer look at the distribution of false positives. The findings show the models as likely to model Dyck(2) if they were trained on the Low LRD corpus, with LSTMs and GRUs achieving promising results.

\section*{Abstract}
Formal grammars, specifically context-free grammars (CFGs), are powerful tools with which to model natural languages. In this thesis, the capability of several recurrent neural networks (RNNs) to learn CFGs by proxy of Dyck(2) was investigated. The impact of training corpus composition was assessed by training models on three different corpora of varying complexity. To assess whether Dyck(2) was learned, the networks classified words into belonging or not belonging to Dyck(2) in two experiments desgined to test their ability to generalize to both extreme long-range dependencies (LRDs) and unseen nesting depths (NDs). Only few RNNs achieved above-chance accuracy. For the ones that did, low training data complexity facilitated generalization, while high complexity showed an inhibitive effect.