\section*{Summary}
%The capability of three major recurrent neural network (RNN) architectures - SRNN, LSTM and GRU - to learn the underlying structure of the Dyck(2) grammar has been investigated. To assess the influence of such factors as model complexity and training corpus composition, each architecture was instantiated in $27$ models, with a model containing $n$ hidden units ($n = \lbrace 2^{1}, 2^{2}, \dots, 2^{9} \rbrace$) each being trained on a character level on one of three corpora. In total, $81$ models were trained and tested. The corpora consisted of a Baseline corpus to compare to, as well as a corpus containing an increased frequency for long-range dependencies (High LRD) and one with a decreased frequency thereof (Low LRD). The models were evaluated on two prediction experiments, designed to assess handling of long-range dependencies and increasing nesting levels. The results were reported in terms of accuracy by character index as well as a cumulative confusion matrix. The findings show the models as capable of modelling large parts of Dyck(2), but incapable of processing recursion by way of nesting depth. Furthermore, the results show a strong influence of corpus composition, as High LRD trained models failed to learn Dyck(2) in all cases, whereas Low LRD trained models always succeeded.

\section*{Abstract}
%Formal grammars, specifically context-free grammars (CFGs), are powerful tools with which to model natural languages. In this thesis, the capability of several recurrent neural networks (RNNs) to learn CFGs by proxy of Dyck(2) was investigated. The impact of training corpus composition was assessed by training models on three different corpora of varying complexity. To assess whether Dyck(2) was learned, the networks predicted words on a character-by-character basis in two tasks designed to test their ability to generalize to higher long-range dependencies (LRDs) and nesting depths (NDs) than encountered in training. Most RNNs were able to generalize well, achieving above-chance accuracy in all cases. The results show that RNNs process recursion as a more difficult problem than LRD. Unable to learn the mechanism of recursion, it seems unlikely that RNNs can learn Dyck(2).