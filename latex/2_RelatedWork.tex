\section{Theoretical Background}\label{ch:theoreticalBackground}
\subsection{Formal Languages and Formal Grammars}\label{formalLanguages}
A formal language $L(G)$ is defined as a subset of all words $\Sigma^{*}$ over an alphabet $\Sigma$, where all words need to comply with the formal grammar $G$. As per \cite{JurafskyMartin2009}, the definition of a formal grammar is $G = \lbrace N, \Sigma, R, S \rbrace$, where $N$ is a set of non-terminal symbols, $\Sigma$ is a set of terminal symbols (alphabet), $R$ is a set of rules of the form $\alpha \rightarrow \beta$ (where $\alpha$ and $\beta$ are strings of symbols from $(\Sigma \cup N)^{*}$) and $S$ is a designated start symbol. Following the two definitions, $L(G)$ consists of all strings $w$ that can be derived from the start symbol $S$ in a finite number of steps, formally $\lbrace w \in \Sigma^{*} | S \xRightarrow [\text{G}]{\text{*}} w \rbrace$. As such, a word $w \in \Sigma^{*}$ that cannot be derived from $S$ in a finite number of steps is not part of $L(G)$.

Formal grammars differ in terms of complexity and can be described in a hierarchical manner. Grammars of higher complexity have a greater generative power than grammars of lower complexity. The most commonly used hierarchy of grammars is the Chomsky hierarchy (\cite{Chomsky1959}). In this hierarchy, formal grammars are classified into four types, sorted from most powerful to least powerful: Turing equivalent (Type 0), Context Sensitive (Type 1), Context Free (Type 2) and Regular (Type 3). The difference in generative power and complexity stems from increasing restrictions imposed on the rules of the grammar - a Type 3 grammar is more restrictive than a Type 0 grammar. As such, every grammar of a higher type is a subset of the previous type of grammar. A visual representation of this property can be found in Figure \ref{fig:ChomskyHierarchy}.

\begin{figure}[htb]
 	\centering
 	\begin{tikzpicture}[scale=0.75, every node/.style={scale=0.75},
	circ/.style={circle, draw, fill, minimum width=3cm},
	]
	
	\node[circ, opacity=0.25, label={[label distance=-1.95cm, align=center]90:Regular \\ (Type 3)}] (Reg) {};
	\node[circ, opacity=0.2, minimum width=5cm, above=0cm of Reg.south, label={[label distance=-1.95cm, align=center]90:Context Free \\ (Type 2)}] (CFG) {};
	\node[circ, opacity=0.15, minimum width=7cm, above=0cm of Reg.south, label={[label distance=-1.95cm, align=center]90:Context Free \\ (Type 1)}]  (CSG) {};
	\node[circ, opacity=0.1, minimum width=9cm, above=0cm of Reg.south, label={[label distance=-1.95cm, align=center]90:Turing Equivalent \\ (Type 0)}] (CSG) {};
	\end{tikzpicture}
	\caption[Chomsky Hierarchy]{A visual representation of the Chomsky Hierarchy.}
\label{fig:ChomskyHierarchy}
\end{figure}

The four types of formal grammars can be defined by the form their rules can take. An overview over these rules as per \cite{JurafskyMartin2009} can be found in Table \ref{tab:GrammarRules}, where $A$ is a single non-terminal, $\alpha$, $\beta$, $\gamma$ are strings of terminal and non-terminal symbols, and $x$ is a string of terminal symbols. $\alpha$, $\beta$ and $\gamma$ may be empty unless specifically disallowed. The table is supplemented with a column describing the corresponding automaton capable of accepting or recognizing the grammar.

\begin{table}[b]
	\begin{tabularx}{\textwidth}{@{}l*{10}{C}c@{}}
	\toprule 
	\textit{Type} & \textit{Name} & \textit{Rule Skeleton} & \textit{Automaton}\\ 
	\toprule	
	0 & Turing Equivalent & $\alpha \rightarrow \beta$, s.t. $\alpha \neq \epsilon$ & Turing Machine (recognized) \\
	1 & Context Sensitive & $\alpha A \beta \rightarrow \alpha \gamma \beta$, s.t. $\gamma \neq \epsilon$ &  Linear Bound Automata (accepted) \\
	2 & Context Free & $A \rightarrow \gamma$ & Push Down Automata (accepted) \\
	3 & Regular & $A \rightarrow xB$ or $A \rightarrow x$ & Finite-State Automata (accepted) \\
	\bottomrule	
	\end{tabularx}
	\caption[Formal grammar properties.]{Overview of formal grammar properties according to \cite{JurafskyMartin2009}, augmented with corresponding automata.}
	\label{tab:GrammarRules}
\end{table}

\subsection{Formal Grammars and Natural Language}
The correspondence of formal grammars to automata (i.e. Kleene's Theorem for regular languages and finite automata) and Computational Complexity Theory lends itself to consider natural languages under the same lense. While formal grammars constitute powerful tools with which phenomena in natural language can be modelled, assessing the precise complexity of Natural Language is the subject of ongoing investigation (\cite{Fitch2012}, \cite{Petersson2012}, \cite{Newmeyer2014}).
Arguments answering that question usually seek to establish lower bounds: If there is a phenomenon in a natural language that cannot be described with a given type of grammar, natural language must be - however slightly - more complex than that type allows. Such arguments increase in credibility the more frequently they can be replicated for phenomena in multiple languages. The arguments establishing natural languages as supra-context-free (i.e. more complex than CFGs) as well as contrary evidence from empiric research shall be presented here.

\subsubsection{Natural Language as supra-regular}\label{supraReg}
English, as well as several other languages (\cite{Hagege1976}) allow for center embedding, the embedding of a phrase into another phrase of the same type.
\begin{exe}
	\ex The man eats.
	\ex The man the boss fired eats.
	\ex The man the boss the investor distrusted fired eats.
	\ex The man the boss the investor the police investigated distrusted fired eats.
\end{exe}
Let the set $E$ contain all grammatical sentences of English, and let the noun phrases and transitive verbs constitute following sets:
\begin{align*}
A &= \lbrace \text{the boss}, \, \text{the investors}, \, \text{the police}, \dots \rbrace \\
B &= \lbrace \text{fired}, \, \text{distrusted}, \, \text{investigated}, \dots \rbrace
\end{align*}
Then the following two sets can be defined.
\begin{align*}
E' &= \lbrace \text{the man } a^{n}b^{n} \text{ eats} \: \vert \: n \geq 0 \rbrace \\
R &= \lbrace \text{the man } a^{*}b^{*} \text{ eats} \rbrace
\end{align*}
$a^{n}$ and $b^{n}$ are finite sequences of size $n$ of elements of sets $A$ and $B$, respectively. $E'$ describes a subset of $E$, namely $E \cap R$. Since regular languages are closed under intersection and $E'$ is not regular, $E$ is not regular.\footnote{The proofs for regular languages being closed under intersection and $E'$ not being regular can be found in \cite{Hopcroft2006} and \cite{Sipser2013}.}

While this proof is correct under the framework of Formal Language Theory, the validity of claiming that it shows natural language to be supra-regular is debatable. Research in psycholinguistics shows that native speakers have faced severe problems processing center embeddings of depth two or higher, yielding long processing times, an incomplete understanding of the presented sentence or leading the participants to judge the sentence as ungrammatical (\cite{Hamilton1971}, \cite{Frank2016}). Furthermore, the corpus-driven analysis by \cite{Karlsson2007} suggests an upper limit of center embedding depth three in the seven investigated languages.
% Keeping the following rant for posterity (or a future blog post)
%The proof concerns linguistic competence, whereas empirical studies provide an insight into linguistic performance. Usually, the distinction between competence and performance provide an adequate framework for both theoretical and empirical work, but when discussing the complexity of natural language neither side should be discarded. A linguistic competence framework cannot appropriately declare natural language as supra-regular if natural language is produced and processed according to regular rules. Conversely, for the purposes of computational linguistics, exclusively assessing linguistic performance by way of corpus analysis is insufficient, as a corpus of any size pales in comparison to the potential for infinitely many combinations that characterizes language. For the purposes of NLP, it is recommendable to err on the side of overestimating the complexity of natural language - so long as the resulting models are sufficiently efficient and accurate.

\subsubsection{Natural Language as supra-context-free}\label{supraCF}
Similarly to the proof given in Section \ref{supraReg}, an argument characterizing natural language as supra-context-free can be brought forth. It is based on embedded infinitival verb phrases found in Swiss German (\cite{Shieber1987}).
\begin{exe}
	\ex
	\gll Jan säit das mer em Hans es huus haend wele hälfe aastriiche. \\
	Jan said that we the Hans-DAT the house-ACC have wanted help paint \\
	\trans 'Jan said that we have wanted to help Hans paint the house.'
	\ex
	\gll Jan säit das mer d'chind em Hans es huus haend wele laa hälfe aastriiche. \\
	Jan said that we the children-ACC the Hans-DAT the house-ACC have wanted let help paint \\
	\trans 'Jan said that we have wanted to let the children help Hans paint the house.'
\end{exe}
Four finite sets can be constructed from these examples: accusative noun phrases ($A = \lbrace \text{d'chind}, \, \dots \rbrace$), dative noun phrases ($B = \lbrace \text{em Hans}, \, \dots \rbrace$), verbs taking accusative objects ($C = \lbrace \text{laa}, \, \dots \rbrace$) and verbs taking dative objects ($D = \lbrace \text{hälfe}, \, \dots \rbrace$). Let the set $S$ then be the set of all grammatical sentences of Swiss German. Again, the two following sets can be defined:
\begin{align*}
S' &= \lbrace \text{Jan säit das mer } a^{n}b^{m} \text{ es huus haend wele } c^{n}d^{m} \text{ aastriiche} \: \vert \: n, m \geq 0 \rbrace \\
R &= \lbrace \text{Jan säit das mer } a^{*}b^{*} \text{ es huus haend wele } c^{*}d^{*} \text{ aastriiche} \rbrace
\end{align*}
$S'$ is not context-free and results from $S \cap R$. Since context-free sets are closed under intersection with regular sets, $G$ cannot be context-free.\footnote{The respective proofs for $S'$ not being context-free and context-free sets being closed under intersection with regular sets can be found in \cite{Hopcroft2006} and \cite{Sipser2013}.}

Curiously enough, empirical research into the matter of processing similar cross-serial dependencies in Dutch suggests them to be generally easier to process than nested dependencies (i.e. the ones used to prove natural language to be supra-regular) (\cite{Bach1986}).

\subsection{Dyck Languages}\label{dyckLanguages}
Whether natural language is regular, context-free, supra-regular or supra-context-free is a distinction of only tangential relevance for this work. The first two cases are fully covered by CFGs, while the other two leave room for some natural language productions outside of the scope of CFGs. The characteristics of supra-context-free examples in natural language show a \textit{weak} non-context-freeness, making CFGs sufficient for covering the vast majority of natural language productions. With this assumption, an appropriate CFG for a model to learn must be found. The most important property of this grammar is that model performance on its language must allow for strong conclusions about the learnability of any other CFG. In doing so, one can make reasoned assumptions about potential model performance on natural language data. 

One such grammar is the Dyck Grammar, which can produce an array of Dyck Languages. Let $D_{n} = \lbrace N, \Sigma, R, S \rbrace$ with
\begin{align*}
	N &= \lbrace S \rbrace \\
	\Sigma &= \lbrace \epsilon, O_{1}, O_{2}, \dots, O_{n}, C_{1}, C_{2}, \dots, C_{n} \rbrace \\
	R &= \lbrace \\
		 & \qquad S \rightarrow \epsilon \\
		 & \qquad S \rightarrow SS \\
		 & \qquad S \rightarrow O_{n} \, S \, C_{n} \, \rbrace ,
\end{align*}
where $O_{n}$ represents an opening parenthesis, $C_{n}$ represents a closing parenthesis and $n$ denotes the number of distinct pairs of parentheses. $D_1$, then, denotes the Dyck Language with $\Sigma = \lbrace \epsilon, (, ) \rbrace$, $D_2$ the Dyck Language with $\Sigma = \lbrace \epsilon, (, [, ], ) \rbrace$, et cetera.

Within the family of Dyck Languages, $D_2$ is of particular interest. According to the Chomsky-Schützenberger Representation Theorem \citep{Chomsky1963}, for every context-free language $L$ there exists a positive integer $n$, a regular language $R$, and a homomorphism $h$ so that $L = h(D_{n} \cap R)$. Following the proof in \cite{Autebert1997}, a homomorphism $g_{n}$ can be constructed so that $D_{n} = g_{n}^{-1}(D_{2})$. It follows that every context-free language can be represented as $L = h(g_{n}^{-1}(D_{2}) \cap R)$. As such, every CFL could be represented via homomorphisms on $D_2$ and intersections with a regular language. Assuming natural languages to be context-free and bearing in mind that using a formal language is a choice of abstraction which allows for precise control over corpus composition, this makes $D_2$ the language of choice when comparing neural network performance.

\subsection{Neural Network Architectures}\label{neuralNetworkArchitectures}
\subsubsection{Simple RNN}\label{SRNN}
Recurrent Neural Networks (RNNs) (\cite{Elman1990}) are a neural network architecture particularly suited to processing sequential information by design: the RNN's output at a time step $t$ is fed back as its input at the following time step $t+1$. Not only does this enable RNNs to process sequences of arbitrary length, it also makes every output dependent on the previous computation as well as the current input. This property equips RNNs with a "memory" for previous inputs, allowing them to capture context dependencies a context-agnostic model cannot adequately learn.

Within the frame of this work, the specific case of the Simple RNN (SRNN) is considered. It is a three layer networks, consisting of an input layer, a hidden layer and an output layer. The hidden state $h_t$ at time step $t$ given the input vector $x_t$ and the output vector $y_t$ are calculated as per the following equations:
\begin{align}
	h_{t} &= f(\boldsymbol{W_{xh}} x_{t} + \boldsymbol{W_{hh}} h_{t-1}) \\
	y_{t} &= \boldsymbol{W_{hy}} h_{t}
\end{align}
The function $f$ constitutes a non-linear transformation, like $\tanh$ or ReLU. $\boldsymbol{W_{xh}}$, $\boldsymbol{W_{hh}}$, $\boldsymbol{W_{hy}}$ are matrices of the weights connecting the input layer to the hidden layer, the hidden layer to itself and the hidden layer to the output layer, respectively.

When training RNNs, it is beneficial to think of the network as unfolding into an architecture with one layer per time step. A visualisation is provided in Figure \ref{fig:unrolledRNN}. These conceptual layers share their parameters - if any weight changes at time step $t$, the weight also changes at $t+1, t+2, \dots, t+n$. Isolated changes are not possible. A popular training algorithm for RNNs is Backpropagation Through Time (BPTT) (\cite{Williams1998}), a gradient based algorithm designed for recurrent rather than feedforward networks. However, as \cite{Bengio1994} and \cite{Hochreiter1998} show, RNNs suffer from a fundamental flaw: the aptly named vanishing gradient problem, in which the training gradient diminishes to zero throughout the layers.

\begin{figure}[htb]
	\begin{tikzpicture}[
    gt/.style={circle, draw, minimum width=1cm, minimum height=3mm, inner sep=1pt},
    ]
   	% Folded nodes
   	% Input
   	\node[gt] (X) {$X$};
   	% Hidden layer
	\node[gt, above=2cm of X.south] (h) {$h$};
	% Output layer
	\node[gt, above=2cm of h.south] (Y) {$Y$};
	
	% Unfolded nodes: t-1
   	% Input
   	\node[gt, right=5cm of X.west] (X_t-1) {$X_{t-1}$};
   	% Hidden layer
	\node[gt, right=5cm of h.west] (h_t-1) {$h_{t-1}$};
	% Output layer
	\node[gt, right=5cm of Y.west] (Y_t-1) {$Y_{t-1}$};
	% Unfolded nodes: t
   	% Input
   	\node[gt, right=2cm of X_t-1.west] (X_t) {$X_{t}$};
   	% Hidden layer
	\node[gt, right=2cm of h_t-1.west] (h_t) {$h_{t}$};
	% Output layer
	\node[gt, right=2cm of Y_t-1.west] (Y_t) {$Y_{t}$};
	% Unfolded nodes: t+1
   	% Input
   	\node[gt, right=2cm of X_t.west] (X_t+1) {$X_{t+1}$};
   	% Hidden layer
	\node[gt, right=2cm of h_t.west] (h_t+1) {$h_{t+1}$};
	% Output layer
	\node[gt, right=2cm of Y_t.west] (Y_t+1) {$Y_{t+1}$};

	% Implied prev/past nodes
	\node[left=1.4cm of h_t-1.west] (dots1) {\dots};
	\node[right=2cm of h_t+1.west] (dots2) {\dots};
	
	% Folded edges
	\draw[-latex] (X) -- (h) node[midway, right] {\small $\boldsymbol{W_{xh}}$};
	\draw[-latex] (h) -- (Y) node[midway, right] {\small $\boldsymbol{W_{hy}}$};
	\draw[-latex] (h) to [out=110, in=230 ,loop ,looseness=4.0] (h) node[left=0.3cm of h] {\small $\boldsymbol{W_{hh}}$};
	
	% Unfold arrow
	\draw[thick, -latex] (0.75, 2) -- (dots1) node[midway, above] {Unfold};
	
	% Unfolded edges t+1
	\draw[-latex] (X_t-1) -- (h_t-1) node[midway, right] {\small $\boldsymbol{W_{xh}}$};
	\draw[-latex] (h_t-1) -- (Y_t-1) node[midway, right] {\small $\boldsymbol{W_{hy}}$};
	
	% Unfolded edges t
	\draw[-latex] (X_t) -- (h_t) node[midway, right] {\small $\boldsymbol{W_{xh}}$};
	\draw[-latex] (h_t) -- (Y_t) node[midway, right] {\small $\boldsymbol{W_{hy}}$};
	
	% Unfolded edges t+1
	\draw[-latex] (X_t+1) -- (h_t+1) node[midway, right] {\small $\boldsymbol{W_{xh}}$};
	\draw[-latex] (h_t+1) -- (Y_t+1) node[midway, right] {\small $\boldsymbol{W_{hy}}$};
	
	% Unfolded edges h->h
	\draw[-latex] (dots1) -- (h_t-1) node[midway, above]  {\small $\boldsymbol{W_{hh}}$};
	\draw[-latex] (h_t-1) -- (h_t) node[midway, above]  {\small $\boldsymbol{W_{hh}}$};
	\draw[-latex] (h_t) -- (h_t+1) node[midway, above]  {\small $\boldsymbol{W_{hh}}$};
	\draw[-latex] (h_t+1) -- (dots2) node[midway, above]  {\small $\boldsymbol{W_{hh}}$};
	\end{tikzpicture}
 	\centering
 	\caption[Unfolded RNN]{An RNN, unfolded through time.}
	\label{fig:unrolledRNN}
\end{figure}

\subsubsection{LSTM}\label{LSTM}
Long-Short Term Memory networks (LSTM) were designed by \cite{Hochreiter1997} as an RNN architecture which preserves the RNN capabilities of processing sequential data of arbitrary length and capturing context dependencies, while circumventing the vanishing gradient problem.

LSTMs are based on self-connected linear units which are regulated by three gates consisting of a sigmoid layer $\sigma$ each: input (in), output (out) and forget (forget). At every time step, the concatenated vector of the previous hidden state $h_{t-1}$ and the current input $x_{t}$ are received by all three gates. The sigmoid layer transforms every value in the concatenated vector to a value in range $[0, \dots, 1]$ - a $0$ translates to forgetting the information, while a $1$ passes it through completely. Thus, the output of the gates determines what information is let through the input gate, passed through the output gate or forgotten by the self-connected linear unit.
\begin{align*}
\text{in}_{t} &= \sigma_{\text{in}} (\boldsymbol{W_{\text{in}}} \cdot [h_{t-1},x_{t}] + b_{\text{in}}) \\
\text{out}_{t} &= \sigma_{\text{out}} (\boldsymbol{W_{\text{out}}} \cdot [h_{t-1},x_{t}] + b_{\text{out}}) \\
\text{forget}_{t} &= \sigma_{\text{forget}} (\boldsymbol{W_{\text{forget}}} \cdot [h_{t-1},x_{t}] + b_{\text{forget}})
\end{align*}
Finally, the cell state $C_{t-1}$ is updated to $C_{t}$ and $h_{t}$ is set.
\begin{align*}
C_{t} &= \text{forget}_{t} \odot C_{t-1} + \text{in}_{t} \odot \tanh (\boldsymbol{W_{C}} \cdot [h_{t-1},x_{t}] + b_{\text{C}}) \\
h_{t} &= \text{out}_{t} \odot \tanh (C_{t})
\end{align*}
\begin{figure}[htb]
	\centering
	\begin{tikzpicture}[
    vec/.style={circle, draw, minimum width=1.2cm},
    op/.style={rectangle, draw, minimum width=0.5cm, minimum height=0.5cm},
    ]
    
   % Full LSTM
   \node[vec] (X) {$X$};
   \node[vec, below=2cm of X.south] (h_t-1) {$h_{t-1}$};
   
   \node[op, below right=1cm of X] (concat) {concat};
   
   \node[op, right=1cm of concat] (forget) {$\sigma$};
   \node[above=0.2cm of forget] (m_forget) {$\text{forget}$};
   \node[op, above=1cm of forget] (in) {$\sigma$};
   \node[above=0.2cm of in] (m_in) {$\text{in}$};
   \node[op, below=1cm of forget] (out) {$\sigma$};
   \node[above=0.2cm of out] (m_out) {$\text{out}$};
   
   \node[op, right=1cm of forget] (C_t) {$C_t$};
   \node[op, right=1cm of C_t] (h_t) {$h_{t}$};
   
   \node[vec, above=2cm of h_t] (C_t-1) {$C_{t-1}$};
   
   \draw[-latex] (X) -- (concat.north west);
   \draw[-latex] (h_t-1) -- (concat.south west);
   \draw[-latex] (concat.north east) -- (in);
   \draw[-latex] (concat) -- (forget);
   \draw[-latex] (concat.south east) -- (out);
   
   % Gate multiplicators.
   \draw[-latex] (m_in) -- (in);
   \draw[-latex] (m_forget) -- (forget);
   \draw[-latex] (m_out) -- (out);
   
   \draw[-latex] (in) -- (C_t.north west);
   \draw[-latex] (forget) -- (C_t);
   \draw[-latex] (out) -- (h_t.south west);
   
   \draw[-latex] (C_t) -- (h_t);
   
   \draw[-latex] (C_t-1.south) to [out=180,in=90] (C_t.north);
   \draw[-latex] (C_t.north) to [out=360,in=270] (C_t-1.south);
   
   \draw[-latex] (X) -- ++(0,2cm) -| (C_t.north west) node[pos=0.25, above] (W_c) {$\boldsymbol{W_{C}}$};
   
   \draw[-latex] (h_t) -- ++(0,-3cm) -| (h_t-1);
   
   % C_t close-up.
   \node[op, right=1cm of C_t-1] (tanh_c) {$\tanh$};
   \node[op, right=1cm of tanh_c] (mult_c1) {$\odot$};
   \node[op, below=1cm of mult_c1.south] (mult_c2) {$\odot$};
   \node[op, below right=0.33cm of mult_c1] (add_c) {$+$};
   \node[draw, dashed, fit=(tanh_c) (mult_c1) (mult_c2) (add_c), label={$C_{t}$}] {};
   \node[scale=0.5, above=0.25 of mult_c1] (in_c) {in};
   \node[scale=0.5, above=0.25 of tanh_c] (x_c) {$X$};
   \node[scale=0.5, left=0.25 of mult_c2] (forget_c) {forget};
   \node[scale=0.5, above=0.25 of mult_c2] (C_t-1_c) {$C_{t-1}$};
   \node[scale=0.5, right=0.5 of add_c] (h_t_c) {$h_{t}$};
      
   \draw[-latex] (x_c) -- (tanh_c.north);
   \draw[-latex] (in_c) -- (mult_c1);
   \draw[-latex] (forget_c) -- (mult_c2);
   \draw[-latex] (C_t-1_c) -- (mult_c2);
   
   \draw[-latex] (tanh_c) -- (mult_c1);
   \draw[-latex] (mult_c1) -- (add_c);
   \draw[-latex] (mult_c2) -- (add_c);
   
   \draw[-latex] (add_c) -- (h_t_c);
   \draw[-latex] (add_c) -- (C_t-1_c);
   
   % h_t close-up.
   \node[op, below=3cm of tanh_c] (tanh_h) {$\tanh$};
   \node[op, right=1cm of tanh_h] (mult_h) {$\odot$};
   \node[scale=0.5, left=0.25 of tanh_h] (C_h) {$C_{t}$};
   \node[scale=0.5, below left=0.25 of mult_h] (out_h) {out};
   \node[scale=0.5, below=0.5 of mult_h] (h_t-1_h) {$h_{t-1}$};
   \node[draw, dashed, fit=(tanh_h) (mult_h), label={$h_{t}$}] {};
   \draw[-latex] (C_h) -- (tanh_h);
   \draw[-latex] (out_h) -- (mult_h);
   
   \draw[-latex] (tanh_h) -- (mult_h);
   
   \draw[-latex] (mult_h) -- (h_t-1_h);
   
\end{tikzpicture}
	\caption[LSTM Memory Cell]{An LSTM memory cell.}
	\label{fig:memoryCellLSTM}
\end{figure}

\subsubsection{GRU}\label{GRU}
A less complex alternative to LSTMs, the Gated Recurrent Unit (GRU) was developed by \cite{Cho2014}. The information flow within the GRU is handled by just two gates: reset ($r$) and update ($z$). The update gate determines how much information from previous time steps is passed along for further time steps, while the reset gate enables the model to drop irrelevant information and only consider the current input rather than the previous hidden state, as described in the equations below, where $j$ is the $j$-th hidden unit, $\sigma$ is the squashing sigmoid function, $\boldsymbol{W}$ and $\boldsymbol{U}$ are learned gate-dependent weight matrices and $\phi$ is a non-linear function.
\begin{align*}
r_j &= \sigma \big( [\boldsymbol{W_{r}}x]_{j} + [\boldsymbol{U_{r}}h_{t-1}]_{j} \big) \\
z_j &= \sigma \big( [\boldsymbol{W_{z}}x]_{j} + [\boldsymbol{U_{z}}h_{t-1}]_{j} \big) \\
h_{j}^{t} &= z_{j}h_{j}^{t-1} + (1 - z_{j}) \tilde{h}_{j}^{t} \\
\tilde{h}_{j}^{t} &= \phi \big( [\boldsymbol{W}x]_{j} +[\boldsymbol{U}(r \odot h_{t-1})]_{j} \big)
\end{align*}

\begin{figure}[htb]
	\centering
	\begin{tikzpicture}[
    	vec/.style={circle, draw, minimum width=1cm},
    	op/.style={rectangle, draw, minimum width=0.5cm, minimum height=0.5cm},
    ]
    % Full GRU
    \node[vec] (X) {$X$};
    \node[vec, below=2.5cm of X] (h_t-1) {$h_{t-1}$};
    \node[op, right=1cm of X] (z) {$z$};
    \node[op, right=1cm of h_t-1] (r) {$r$};
    \node[op, right=1cm of r] (h_tilde) {$\tilde{h}$};
    \node[op, above=1.5cm of h_tilde] (h) {$h$};
    \node[vec, right=1cm of h] (Y) {$Y$};
    % Help nodes for clean edges
    \node[below right=1cm of h_t-1] (in_h_t-1) {};
    \coordinate[below=0.5cm of h_tilde] (in_h_tilde);
    
    
    % Edges z
    \draw[-latex] (X) -- (z) node[midway,above] {\small $\boldsymbol{W_{z}}$};
    \draw[-latex] (h_t-1) -- (z) node[sloped, pos=0.80, above] {\small $\boldsymbol{U_{z}}$};
    \draw[-latex] (z) -| (h);
    % Edges r
    \draw[-latex] (X) -- (r) node[sloped, pos=0.65, above] {\small $\boldsymbol{W_{r}}$};
    \draw[-latex] (h_t-1) -- (r) node[midway, above] {\small $\boldsymbol{U_{r}}$};
    \draw[-latex] (r) -- (h_tilde);
    \draw[-latex] (h_tilde) -- (h);
    \draw[-latex] (h_t-1.south east) -- ++(0,-0.5cm) -| (h_tilde.south);
    % Edges h_tilde
    \draw[-latex] (X) -- (h_tilde) node[sloped, midway, above] {\small $\boldsymbol{W}$};
    % Edges h
    \draw[-latex] (h_t-1) -- (h);
    \draw[-latex] (h) -- (Y);
    % Loop Output back to h_t-1
    \draw[-latex] (Y) -- ++(0,-3.5cm) -| (h_t-1.south);
    
    % Operator closeups
    %h closeup
    \node[op, right=0.75cm of Y] (1-) {$1-$};
    \node[op, right=0.5cm of 1-] (h_mult2) {$\cdot$};
    \node[op, below=0.5cm of 1-] (h_mult) {$\cdot$};
    \node[op, right=1cm of h_mult] (+h) {$+$};
    \node[scale=0.5, above=0.25cm of 1-] (z_h) {$z$};
    \node[scale=0.5, below=0.25cm of h_mult2] (h_tilde_h) {$\tilde{h}$};
    \node[scale=0.5, below left=0.25cm of h_mult] (h_t-1_h) {$h_{t-1}$};
    \node[draw, dashed, fit=(1-) (h_mult) (h_mult2) (+h), label={$h$}] {};
    \draw[-latex] (z_h) -- (1-);
    \draw[-latex] (1-) -- (h_mult2);
    \draw[-latex] (h_tilde_h) -- (h_mult2);
    \draw[-latex] (h_t-1_h) -- (h_mult);
    \draw[-latex] (h_mult) -- (+h);
    \draw[-latex] (h_mult2) -- (+h);
    \draw[-latex] (+h) -- ++(0.5cm,0);
    
    %h_tilde closeup
    \node[op, below=1cm of h_mult] (h_tilde_mult) {$\odot$};
    \node[op, right=0.5cm of h_tilde_mult] (+h_tilde) {$+$};
    \node[op, right=0.5cm of +h_tilde] (lin_h_tilde) {$\phi$};
    \node[scale=0.5, above left=0.25cm of +h_tilde] (x_h_tilde) {$X$};
    \node[scale=0.5, left=0.25cm of h_tilde_mult] (r_h_tilde) {$r$};
    \node[scale=0.5, below left=0.25cm of h_tilde_mult] (h_t-1_h_tilde) {$h_{t-1}$};
    \node[draw, dashed, fit=(h_tilde_mult) (+h_tilde) (lin_h_tilde), label={$\tilde{h}$}] {};
    \draw[-latex] (r_h_tilde) -- (h_tilde_mult);
    \draw[-latex] (h_t-1_h_tilde) -- (h_tilde_mult);
    \draw[-latex] (x_h_tilde) -- (+h_tilde.north west);
    \draw[-latex] (h_tilde_mult) -- (+h_tilde) node[midway, above] {\tiny $\boldsymbol{U}$};
    \draw[-latex] (+h_tilde) -- (lin_h_tilde);
    \draw[-latex] (lin_h_tilde) -- ++(0.5cm,0);
    
    % z
    \node[op, above=0.75cm of 1-] (+z) {$+$};
    \node[op, right=0.5cm of +z] (sigz) {$\sigma$};
    \node[scale=0.5, above left=0.25cm of +z] (x_z) {$X$};
    \node[scale=0.5, below left=0.25cm of +z] (h_t-1_z) {$h_{t-1}$};
    \node[draw, dashed, fit=(+z) (sigz), label={$z$/$r$}] {};
    \draw[-latex] (x_z) -- (+z.north west);
    \draw[-latex] (h_t-1_z) -- (+z.south west);
    \draw[-latex] (+z) -- (sigz);
    \draw[-latex] (sigz) -- ++(0.5cm,0);
\end{tikzpicture}
 \caption[Illustration of a GRU]{Illustration of a GRU.}
\label{fig:GRU}
\end{figure}

\subsection{Related Works}\label{relatedWorks}
Currently, NLP models are trained and tested on vast datasets, such as the CoNLL Shared Tasks. By evaluating a variety of different models and approaches on the same data, it is possible to easily assess which one poses the current state-of-the-art for any given NLP task.

The earliest research on neural network architectures was done before such datasets were widely available and easy to process (\cite{Cleeremans1989}, \cite{Elman1990}, \cite{Zeng1994}, \cite{Hochreiter1997}, \cite{Rodriguez1998}, \cite{GersSchmidhuber2001}). During that time, novel architectures and algorithms were mostly scored on formal language datasets, with the test set containing longer words than the training set to assess learning success. However, evaluating on formal languages comes with its own advantages and challenges.

Primarily, it is undeniably cheaper than scoring on a natural language dataset. By deriving words from the grammar, datasets of arbitrary length with arbitrary properties can be generated. However, performance on a formal grammar dataset should always be understood as a simplified benchmark. As mentioned in Sections \ref{supraReg} and \ref{supraCF}, the formal complexity of natural language is debatable, limiting the significance of formal benchmark performance for NLP tasks. Nevertheless, formal language datasets are still used to evaluate the performance of novel architectures to this day, as done by \cite{Joulin2015}, \cite{Bernardy2018}, \cite{Deleu2016}, \cite{Li2018} and \cite{Yu2019}.

In addition to the exploration of new architectures, formal languages are also still used to investigate particular behaviours of well-established architectures, such as LSTMs (\cite{Sennhauser2018}), or to compare several established models on a specific set of tasks (\cite{Skachkova2018}, \cite{Suzgun2019}).
\begin{table}
	\begin{tabularx}{\textwidth}{@{}l*{10}{C}c@{}}
		\toprule 		
		\textit{Paper} & \textit{D$_n$} & \textit{Grammar Probability} & \textit{Training Corpus Size} \\ 
		\toprule
		\cite{Deleu2016} & 1 & equal & unclear \\ 
		\cite{Bernardy2018} & 1, 5 & equal & $102,400$ \\ 
		\cite{Li2018} & 1 & modified & $200$ - $20,000$ \\ 
		\cite{Skachkova2018} & 1-5 & modified & $131,072$ \\ 
		\cite{Sennhauser2018} & 2 & modified & $1,000,000$ \\ 
		\cite{Suzgun2019} & 1-2 & modified & $10,000$ \\ 
		\cite{Yu2019} & 2 & modified & $1,000,000$ \\ 
		\bottomrule
	\end{tabularx}
	\caption[Corpus sizes in current works]{Overview of corpus sizes in current works.}
	\label{tab:LiteratureCorpusOverview}
\end{table}
\begin{table}
	\begin{tabularx}{\textwidth}{@{}l*{10}{C}c@{}}
	\toprule 
	\textit{Paper} & \textit{Accuracy} & \textit{Perplexity} & \textit{Cell State} & \textit{AUC} & \textit{Error Rate} \\
	\toprule
	\cite{Deleu2016} & No & No & No & Yes & No \\
	\cite{Bernardy2018} & Yes & No & No & No & No \\
	\cite{Skachkova2018} & Yes & Yes & No & No & No \\
	\cite{Sennhauser2018} & No & No & Yes & No & Yes \\
	\cite{Suzgun2019} & Yes & No & Yes & No & No\\
	\cite{Yu2019} & No & Yes & No & No & Yes \\ 
	\bottomrule
	\end{tabularx} 
	\caption[Reported values for performance in previous works]{Overview of reported values for performance. Cell State Analysis does not refer to a unified method, it merely means the paper investigates cell states at all. AUC refers to the area under the curve for an increasing length of Dyck words the model was able to generalize.}
	\label{tab:LiteratureReportedMeasures}
\end{table}
\begin{table}
	\begin{tabularx}{\textwidth}{@{}l*{10}{C}c@{}}
	\toprule 
	\textit{Paper} & \textit{Architectures} \\
	\toprule 
	\cite{Deleu2016} & Neural Turing Machine, LSTM \\
	\cite{Bernardy2018} & GRU, LSTM, RUSS \\
	\cite{Skachkova2018} & SRNN, GRU, LSTM \\
	\cite{Sennhauser2018} & LSTM \\
	\cite{Suzgun2019} & SRNN, GRU, LSTM \\
	\cite{Yu2019} & seq2seq \\ 
	\bottomrule
	\end{tabularx}
	\caption[Overview of investigated models]{Overview of investigated models.}
	\label{tab:LiteratureInvestigatedModels}
\end{table} 

While all these papers use a formal language to evaluate models, several factors prevent them from forming a solid basis upon which to compare their respective results:
First, there is neither a benchmark train/dev/test set for Dyck languages (as is standard for most machine learning tasks) (Table \ref{tab:LiteratureCorpusOverview}), nor a set of measures that is reported consistently throughout the literature (Table \ref{tab:LiteratureReportedMeasures}). Additionally, only two papers compare the three well-established architectures (SRNN, GRU, LSTM) directly (Table \ref{tab:LiteratureInvestigatedModels}).
Finally, the employed training and test measures are not unified - in \cite{Bernardy2018}, for example, the models are trained to predict the next letter in words of variable length at any given time step, while in \cite{Suzgun2019}, they predict the final letter of a word. Further model details, such as the inclusion of an Embedding and/or a Dropout layer or the number of hidden units also vary.

In conclusion, despite formal languages having been used to assess neural network model performance for decades, there is little to no comparative studies of SRNN, LSTM and GRU models performing on $D_{2}$. Any research comparing new architectures to any of these models does so with both varying training and testing methods, as well as with vastly differing corpora and as such cannot be directly compared with each other. This allows for no conclusive statement on the relative performance of these popular RNN architectures based on the current literature.