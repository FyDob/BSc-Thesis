\section{Conclusion}\label{ch:conclusion}
This work has set out to answer three questions, as posed in Chapter \ref{ch:introduction}. Related literature has been consulted to choose a proper approach. However, current literature contains neither a benchmark dataset to train and test models on, nor a unified set of tasks and measures to do so. Due to these facts, most results in current literature discussing model performance on $D_{2}$ are incomparable to each other.

To assess model performance, I have adapted the two experiments proposed by \cite{Bernardy2018} for a classification task. They were explicitly designed to investigate model performance on long-range dependencies, as well as a model's ability to generalize to deeper nesting depths. I have reported accuracy, precision, recall and the compound measure of F1 score across models and experiments, as well as providing a closer look at common error sources for the models.

In addition to assessing three different architectures on two experiments, I have investigated the impact of hidden unit number and training corpus composition on model performance. For the former, each architecture was implemented in $9$ different models with hidden unit number $n \in \lbrace 2^{1}, 2^{2}, \dots, 2^{9} \rbrace$. All models have been trained with the same hyperparameters. To achieve the latter, I have constructed three training corpora: a baseline corpus (Base), a corpus containing words with a high nesting depth and maximum bracket distance (High LRD) and a corpus containing words with a low nesting depth and maximum bracket distance (Low LRD). All corpora contained $1{,}000{,}000$ words, $500{,}000$ of which were correct. The incorrect words consisted of $250{,}000$ words with extra open and $250{,}000$ words with extra closing brackets. In total, $81$ models were trained, and each model was evaluated on both experiment data sets.

The results of both experiments show learning $D_{2}$ to be a task of not trivial and volatile difficulty: the number of hidden units in models with an accuracy well above random guessing ranges from $2$ to $512$. A vast majority of models failed to extract any useful information from the training data, either staying near the random decision baseline or vastly underperforming. Successful models mostly exhibit a behaviour indicating they have extracted a basic approximation of $D_{2}$ from the linear training data by way of barely differentiating between the two error categories.

However, processing extreme long-range dependencies spanning the whole word was more difficult for the models than processing extreme unseen nesting depths. %TODO whyyyyyyyyyy
%TODO is the following line still accurate?
%While this behaviour is in line with human performance (\cite{Karlsson2007}), it shows these three architectures to be incapable of perfectly learning $D_{2}$ from any of the three training corpora.

As expected, LSTMs and GRUs outperformed SRNNs. While successful LSTM models achieved the highest individual accuracy scores (topping out at $99.3\%$) and came closest to a perfect open/closed misclassification ratio of $1$, GRUs were more consistently successful, both producing the most models performing better than chance and achieving the highest average accuracy. It stands to reason that, while not succeeding under the circumstances of this study, LSTMs show the highest capability of perfectly learning $D_{2}$. Taking the Chomsky-Sch√ºtzenberger Representation Theorem (\cite{Chomsky1963}) into account, it seems possible that these architectures are capable of learning the vast class of Type-2 languages - which likely contains natural language.

I have found corpus complexity to have a significant impact on model performance. Models trained on the Low LRD corpus largely outperformed the Base corpus models, while High LRD corpus models underperformed. While models trained on complex words failing to generalize to longer, more complex words seems paradoxical at first, these results suggest that whatever rules RNNs extract from the input data, rule extraction becomes harder the more complex the data is. Taking into account the open/closed bracket misclassification ratio, I suggested that RNNs can extract a small number of simple rules from simple data and generally apply them in a more complex context. The more complex training data encourages the models to instead learn a larger set of overly specified rules - and then fail to generalize them. If a Low LRD trained model learns that an open bracket must always be closed by its corresponding closing bracket, the High LRD trained model might learn that an open bracket may be closed by its corresponding closing bracket after a certain number of characters, or only after a certain number of nesting levels have been resolved, leading to a bloated, overspecified and misleading ruleset.

\subsection{Further Research}
The most compelling result of this research is the effect of training corpus complexity on generalizability. While it holds true in this case, formal language data is by definition rigorously structured and, for $D_{2}$, rather simple and limited. Natural language data features a far bigger alphabet, complex syntactic, semantic and morphological dependencies and irregularities. Nonetheless, RNNs for NLP tasks improving with structurally simplified training data poses an intriguing and possibly fruitful avenue of future research.
Furthermore, while LSTMs emerged as the most promising architecture to learn $D_{2}$, they have proven to be more volatile than GRUs. Whether there is an inherent difference between the architectures remains unclear and may be explored with more established and sophisticated methods of internal state analysis.