\section{Conclusion}\label{ch:conclusion}
This work has set out to answer three questions, as posed in Chapter \ref{ch:introduction}. Related literature has been consulted to choose a proper approach. However, current literature contains neither a benchmark dataset to train and test models on, nor a unified set of tasks and measures to do so. Due to these facts, most results in current literature discussing model performance on $D_{2}$ are incomparable to each other.

%To assess model performance, the two experiments proposed by \cite{Bernardy2018} have been used. They were explicitly designed to investigate model performance on long-range dependencies, as well as a model's ability to generalize to deeper nesting depths. Accuracy by index as well as the confusion matrices of the evaluated models were reported.
%
%In addition to assessing three different architectures on two experiments, the impact of hidden unit number and training corpus composition on model performance has been investigated. For the former, each architecture was implemented in $9$ different models with hidden unit number $n \in \lbrace 2^{1}, 2^{2}, \dots, 2^{9} \rbrace$. To achieve the latter, three training corpora have been constructed: a baseline corpus (Baseline), a corpus containing words with a high nesting depth and maximum bracket distance (High LRD) and a corpus containing words with a low nesting depth and maximum bracket distance (Low LRD). In total, $81$ models were trained and evaluated.
%
%By analyzing the results of both experiments on the best performing models per architecture and corpus, it has been shown that all three architectures are capable of learning an approximate representation of $D_{2}$ with a low number of hidden units. However, this representation fails to account for infinite recursion, as is apparent by the model performance in Experiment 2. Increasing the number of hidden units does not improve model performance.
%
%While this behaviour is in line with human performance (\cite{Karlsson2007}), it shows these three architectures to be incapable of perfectly learning $D_{2}$ from any of the three training corpora. Taking the Chomsky-Sch√ºtzenberger Representation Theorem (\cite{Chomsky1963}) into account, it seems improbable that these architectures are capable of learning the vast class of Type-2 languages - which likely contains natural language.
%
%Corpus composition has been found to have a significant impact on model performance. Models trained on the High LRD corpus completely fail at learning the grammar underlying the data, while models trained on the Low LRD corpus do so in all cases. Indeed, training on the Low LRD corpus induces the capability to generalize properly in GRUs, which have shown to be the worst model at recognizing the underlying grammar.
%
%To finally answer the questions posed in Chapter \ref{ch:introduction}: None of the architectures could perfectly learn $D_{2}$. The challenge in learning the grammar does not lie in memorising an open bracket over long distances, but rather in processing recursion adequately.
%Corpus construction has a tangible effect on model performance - presenting small, low-complexity words during training improves generalizability, while presenting overly complex words similar to the data used in experiments drastically worsens it.
%
%These results raise several questions that may be pursued in further research. First, seeing that recursion and long-range dependency is processed differently indicates some underlying structure being learned - otherwise there should, by all accounts, be no difference between the two phenomena. What exactly that structure is and how it corresponds to $D_{2}$ remains unanswered. Furthermore, investigating the internal states of the models might explain why the GRU is severely outperformed by the LSTM when it comes to learning the grammar. Additionally, the impact of corpus composition could be applicable to further machine learning research concerning structural time series data generated by an unknown system. Further research on manipulated Natural Language training data might prove fruitful in confirming the applicability of these results to NLP research.