%TODO talk about 0.0 F1 score models
%TODO introduce the +/- 5% corridor in the first paragraph to disregard the insignificant results.
\section{Results}\label{ch:results}
During training, almost all $81$ trained models achieved a validation accuracy near $100\%$, except for the SRNN-2 models trained on the base and low LRD corpus, which scored $75.0\%$ and $50.4\%$ respectively. I have included them in the experiments regardless of their low validation accuracy, since it was unclear whether validation accuracy would be a strong predictor for a network's performance on the experiment data. I present my results with regard to three focus points: First, the overall performance of different architectures with respect to which corpus they were trained on, then the individual model performances on each of the two experiments, and finally a closer look at classifications made by outlier networks - networks which drastically over- or underperformed in either of the experiments - with regards to misclassified false positives.

\subsection{Architecture/Training Data}
\input{tab/results_compounded}
As can be seen in Table \ref{tab:perf_all}, none of the architectures consistently achieved an accuracy far above the random guessing baseline of $50.0\%$. However, there was still a notable difference in performance between architectures: on average, the GRU networks scored the highest on accuracy and precision, while the SRNN networks achieved the best recall and F1 score. With $51.5\%$, LSTMs scored an average accuracy between SRNNs ($50.0\%$) and GRUs ($53.3\%$), but they underperformed in all other experiment measures.

Furthermore, the choice of training data had a notable effect on overall model performance: SRNNs and GRUs received a boost in performance in all measures when comparing the Base to the Low LRD models, elevating SRNNs from an accuracy below random guessing to $51.4\%$. While LSTMs lost $1.4\%$ in terms of accuracy, all other performance measures improved significantly for the Low LRD models. Training on the High LRD corpus aided SRNNs in terms of accuracy, precision and F1 score, but worsened accuracy, recall and F1 score for LSTMs and GRUs.

\subsection{Experiment 1: Long-Range Dependency}\label{resultsLRD}
I report results for Experiment 1 in Tables \ref{tab:perf_LRDbase}, \ref{tab:perf_LRDlow} and \ref{tab:perf_LRDhigh}, which include the performance measures for all networks trained on the Base, Low and High LRD corpus respectively, as evaluated on Experiment 1.

\input{tab/results_LRD_base}

When trained on the Base corpus, $8$ of $27$ models ($29.6\%$) achieved an accuracy above random guessing. Among those $8$, only $2$ reached an accuracy above $60\%$: LSTM-8 and GRU-2. The vast majority of models - $20$ in total - reached an accuracy of $50 \pm 5\%$. $4$ performed even worse than that: the worst model (SRNN-16) only achieved $27.2\%$ accuracy on the experiment data. There was no apparent correlation between validation accuracy and model performance in Experiment 1 - indeed, SRNN-2 with the lowest validation accuracy at $75.0\%$ evaluated at below chance, but so did several models with a validation accuracy of $100\%$.

\input{tab/results_LRD_low}

$12$ of $27$ Low LRD-trained models ($44.4\%$) scored an accuracy higher than the random guessing baseline. Among those, $3$ models - SRNN-4, LSTM-16 and GRU-64 - reached an accuracy above $60\%$. While $22$ models fell within the $50 \pm 5\%$ belt of accuracy, only $1$ model performed significantly worse: LSTM-4 with $27.8\%$. Validation accuracy was entirely unrelated to model performance, with LSTM-4 having achieved a perfect score on the validation data, but completely failing at Experiment 1. On average, all measures have improved when compared to the Base models: accuracy improved by $+2.7$ percentage points (p.p.), precision by $+11.0$ p.p., recall by $+9.5$ p.p. and F1 score by $+8.3$ p.p.

\input{tab/results_LRD_high}

$9$ of $27$ High LRD-trained models ($33.3\%$) achieved an accuracy above baseline, but only GRU-512 by a significant margin with $80\%$. $23$ models performed within the $\pm 5\%$ margin around the baseline, and $3$ models (LSTM-4, LSTM-8 and GRU-8) underperfomed significantly. There was no relation between validation accuracy and performance on experiment data for High LRD models, either. Compared to the Base models, High LRD models almost consistently scored worse: accuracy, recall and F1 score went down by $-1.0$, $-2.2$ and $-1.8$ p.p. respectively, but precision was improved by $+5.6$ p.p.

\subsection{Experiment 2: New Depths}\label{resultsND}
All results for Experiment 2 can be found in Tables \ref{tab:perf_NDbase}, \ref{tab:perf_NDlow} and \ref{tab:perf_NDhigh}. When comparing mean performances across all models, they largely scored higher on Experiment 2 than Experiment 1. This suggests that Experiment 2 was easier regardless of training data. As with Experiment 1, a model's validation accuracy did not correlate with its performance on the experiment data.

\input{tab/results_ND_base}

Among the Base models, $8$ of $27$ ($29.6\%$) performed above guessing baseline in Experiment 2 - the same ratio as for Experiment 1, though there was minimal overlap in the best performers. Only GRU-64 and SRNN-64 performed above $50\%$ accuracy for both experiments. $20$ of $27$ models stayed in the $\pm 5\%$ margin of the baseline, with only $2$ (SRNN-128 and SRNN-256) dropping below that. The best performing network - LSTM-128 - scored the highest accuracy across all models and all experiments with $99.3\%$.

\input{tab/results_ND_low}

The same number of Low LRD models performed above chance for Experiment 2 as for Experiment 1 ($44.4\%$), with $6$ models (SRNN-2, SRNN-8, LSTM-16, LSTM-512, GRU-4 and GRU-64) occuring in both groups. LSTM-512 and GRU-64 have both achieved an accuracy $>90\%$. $19$ models performed within the $\pm 5\%$ margin around the baseline, only $2$ dropped below that. Compared to the Base models, low LRD models also performed better on Experiment 2: mean accuracy is up by $+0.4$ p.p., precision by $+20.6$, recall by $+3.5$ and F1 score by $6.5$ p.p.

\input{tab/results_ND_high}
While the highest number of High LRD models have achieved an accuracy above random guessing - $10$ of $27$, or $37.0\%$ - only SRNN-16 crossed the $60\%$ threshold at all. Indeed, aside from SRNN-16, only one other model lay outside of the $\pm 5\%$ margin around the baseline - LSTM-64, with an accuracy of $28.5\%$. When comparing with the Base models, all measures except for precision, which improved by $+10.7$ p.p. Accuracy went down by $-2.7$ p.p., recall by $-7.5$ and F1 score by $-5.0$ p.p.

\subsection{Outliers: A Closer Look}
As is evident from the results described so far, a vast majority of models fell within a $\pm 5\%$ margin around random guessing in terms of accuracy. I consider these models to not have extracted any useful grammar information from the training data and discard them for further investigation. As such, I will only discuss models with an accuracy either $> 55\%$ or $< 45\%$, to both gather information on what caused models to succeed, and what caused them to fail.

\include{tab/openclose_ratio}

First, I report the number of the two error categories (superfluous open bracket, superfluous closed bracket) per network architecture in Tables \ref{tab:opencloseRatios} and. Hypothetically, since both categories were balanced in training and experiment data, there should be no significant difference between misclassifying one or the other - unless one category proves to be more complex to an architecture. The ratio of open-to-closed bracket misclassifications serves as a simple indication of whether the model extracted the correct information - that the amount of brackets needs to be balanced throughout a valid word - at all. A ratio of $1$ implies no model difference between the two categories. Conversely, any skewing above or below $1$ shows the model disproportionally struggling with one of the two categories. Only one model achieved a near perfect ratio: LSTM-16 trained on the Low LRD corpus, evaluating the Experiment 1 data.

Wrong words with superfluous open brackets have shown to be the most difficult to reject: $61.54\%$ of the High Accuracy Models and $80\%$ of the Low Accuracy Models skew towards a ratio $> 1$. Furthermore, a model's inability to treat both error categories the same as indicated by the ratio correlates to model accuracy: will most High Accuracy Models err reasonably closely to $1$, the Low Accuracy Models show a much more extreme distribution, frequently misclassifying almost all words in one error category, while handling the other one perfectly. There is no trend towards one architecture overall skewing towards $1$. However, among the High Accuracy Models, the models trained on the Low LRD corpus have the lowest deviation from $1$, with $0.256$ on Experiment 1, and $0.652$ on Experiment 2. The lowest deviation from $1$ among the Low Accuracy Models comes from the models trained on the Base corpus, with $25.65$ and $9.382$, respectively.

%To investigate properties of wrongly classified words, I scrutinize false positives for two word properties. I chose the properties as proxy values for word complexity: the maximum nesting depth of the word, and the position of the error within the word. Since all words within an experiment have the same length, variation in complexity can only arise through structural differences - like a deeper underlying parse tree. The values were averaged across all model instances with the same architecture and training corpus for reasons of simplicity, i.e. the results of both good base GRU outliers in Experiment 1 were merged. A low value in mean error position means that the model tended to misclassify words where the error occured early on in the word, while a high value means that the error occured later. Maximum nesting depth is an overall measure of complexity for the word, and refers to the deepest nesting level within the word. It is unrelated to the error position. Again, a low value means that the model predominantly misclassified words with a low nesting depth.

%\begin{figure}[!tbp]
%	\begin{minipage}{\textwidth}
%     \centering
%     \includegraphics[width=.45\textwidth]{fig/LRD_error_pos_good}\quad
%     \includegraphics[width=.45\textwidth]{fig/LRD_error_pos_bad}\\
%     \includegraphics[width=.45\textwidth]{fig/LRD_max_valid_nesting_depth_good}\quad
%     \includegraphics[width=.45\textwidth]{fig/LRD_max_valid_nesting_depth_bad}
%     \subcaption{Properties of false positives in Experiment 1.}
%     \label{fig:FP_Prop_LRD}
%   \end{minipage}\\[1em]
%   \begin{minipage}{\textwidth}
%     \centering
%     \includegraphics[width=.45\textwidth]{fig/ND_error_pos_good}\quad
%     \includegraphics[width=.45\textwidth]{fig/ND_error_pos_bad}\\
%     \includegraphics[width=.45\textwidth]{fig/ND_max_valid_nesting_depth_good}\quad
%     \includegraphics[width=.45\textwidth]{fig/ND_max_valid_nesting_depth_bad}
%     \subcaption{Properties of false positives in Experiment 2.}
%     \label{fig:FP_Prop_ND}
%   \end{minipage}\\
%  \caption{Properties of false positives as classified by different architectures. A missing architecture/training corpus combination means that none of these models scored an accuracy outside of $50 \pm  5\%$, and were not included in this analysis.}
%\end{figure}

%confusion matrix?

%As can be seen in Figure \ref{fig:FP_Prop_LRD}, there is a notable difference in average error position, not just between High and Low Accuracy models, but also between architectures and training corpora. The value tends to be highest for models trained on the High LRD corpus, followed by models trained on the Low LRD corpus. The highest accuracy model for Experiment 1 was Base LSTM-8 with an accuracy of $91.00\%$. This model misclassified words with the earliest error occurrence, setting in at $4.417$. For Maximum Nesting Depth, there is no significant difference between training corpus and models, save for a massive spike for Base LSTM-8. Comparing High and Low Accuracy models shows mostly lower Average Error Position values, with the GRU models being notable exemptions to the rule. Low Accuracy models tend to misclassify more complex words.
%
%For Experiment 2, Figure \ref{fig:FP_Prop_ND} shows stark differences in Average Error Position depending on the training corpus. All models trained on the Low LRD corpus misclassify words with a higher Average Error Position than models trained on the Base corpus. As in Experiment 1, the best performing model (Base LSTM-128) scores the lowest Average Error Position and the highest Average Nesting Depth. Comparing High and Low Accuracy models, the Low Accuracy models achieve drastically lower Average Error Positions and slightly lower Average Maximum Nesting Depth.
%
%These values provide little conclusive insight into what properties characterize a successful model. However, they do illustrate an overall altered performance depending on which corpus the model was trained on, indicating that the complexity of the input data has an impact on how the model evaluates unseen, more complex data.