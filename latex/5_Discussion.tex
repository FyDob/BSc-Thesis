\section{Discussion}\label{ch:discussion}
The results presented in Chapter \ref{ch:results} provide an in-depth look at model performance, reporting on established measures like accuracy and F1 score, but also on custom measures for this task, like misclassification ratio for the two categories of incorrect words. All of this was done to answer the question of whether RNNs can learn the underlying structure of $D_{2}$, and whether specific properties of the training data can facilitate or inhibit that ability. I will now interpret the results from the perspective of these questions.

\subsection{Learning $D_{2}$}
Both experiments were designed to test the two most important aspects of what it means to generalize from a small language subset to more complex data: interpreting extreme long-range dependencies on long, unseen words, and handling unseen nesting depths. The former showcases the ability of generalizing to much greater length, while the latter provides insight on how well the model handles deeper, more complex parse trees.

Judging from the average accuracy across all models reported in Tables \ref{tab:perf_LRDbase}, \ref{tab:perf_LRDlow}, \ref{tab:perf_LRDhigh}, \ref{tab:perf_NDbase}, \ref{tab:perf_NDlow} and \ref{tab:perf_NDhigh}, Experiment 1 was a harder task than Experiment 2, regardless of training corpus. By and large, models were struggling to correctly classify extreme long-range dependencies, while performing well on a deeper nesting task with shorter dependencies. Assuming that nesting depth, as it creates deeper and more complicated underlying parse trees than low-nesting depth long-range dependencies, would pose a higher difficulty to models that have learned to build a structural representation and as such, learned $D_{2}$, this discrepancy suggests that most models did not achieve such a representation.

In conclusion, the experiments posed in this thesis were hard tasks. Not many models learned helpful information from the training data, some extracted the wrong kind of information, but most learned nothing to help them improve from random guessing. Successful model hidden unit counts range from $2$ to $512$, with no general trend connecting number of hidden units and model performance, making it impossible to assess lower and upper limits of model complexity required to learn $D_{2}$.

However, $20$ models in total have performed above chance on either of the two experiments. Accuracy ranges from  $80.0\%$ (High GRU-512) to $91.0\%$ (Base LSTM-8) for Experiment 1, and $67.5\%$ (High SRNN-16) to $99.3\%$ (Base LSTM-128) for Experiment 2. Regardless of experiment and training data, GRUs slightly outperformed LSTMs and SRNNs with an average accuracy of $53.5\%$ (see Table \ref{tab:perf_all}).

Individual model performance provides another intriguing implication: Several models that performed well in Experiment 1 barely achieved the baseline in Experiment 2 and vice-versa. This points to the tasks, despite both making a case for generalizability, posing different requirements to the representations the models learned. In fact, only two models performed above baseline for both experiments: Low LSTM-16 ($88.1\%$ and $56.66\%$ for Experiment 1 and 2, respectively) and Low GRU-64 ($89.0\%$ and $93.5\%$). Considering there were $20$ successful models in total, this number is fairly low, further pointing to learning $D_{2}$ being a difficult undertaking. Both of these models have been trained on the Low LRD corpus, indicating that the corpus facilitates all-purpose generalization.

To assess whether a High Accuracy model learned a valid representation of $D_{2}$, incorrect words were split in two equal sized classes: words with superfluous open or closed brackets. A good model, then, should not make a difference between the two classes. While the High Accuracy models did not always succeed at that, they came a lot closer to treating both classes equally than the Low Accuracy models. Furthermore, the imbalance in false positive misclassification was skewed towards words with superfluous open brackets. This might be owed to the fact that an extra closed bracket at any position immediately renders a word of any length ungrammatical: it resolves a dependency that does not exist. An extra open bracket, however, opens a dependency that might be resolved at a later point in the word. Indeed, all incorrect open bracket words are substrings of longer correct $D_{2}$ words. The same cannot be said for incorrect closed bracket words.\footnote{It must be noted that this property makes the incorrect open words in no way more valid than the incorrect closed words. The experimental stimuli are of a fixed length, and their end is - like in the training data - signified by an end-of-word symbol. There is no reason for a model to anticipate an extension to resolve the open dependency.}

In conclusion, the combination of task difficulty and volatile model performance makes it difficult to conclusively compare the three architectures. While GRUs achieved a high accuracy more consistently, LSTMs have produced the highest scoring models. Despite SRNNs scoring the worst in terms of overall accuracy, they - on average - outperformed the other two architectures in terms of F1 score. However, given the measure of misclassification ratio, the record high accuracy and fairly low number of hidden units in well-performing models, LSTMs have shown great promise to be capable of learning $D_{2}$ in this study.

\subsection{Influence of Training Data}
Whether comparing accuracy, precision, recall and F1 score across both experiments (see Table \ref{tab:perf_all}, individually comparing model instances based on what corpus they were trained on (i.e. Tables \ref{tab:perf_LRDbase}, \ref{tab:perf_LRDlow} and \ref{tab:perf_LRDhigh} for Experiment 1) or looking for a trend in open/closed misclassification ratio (see Table \ref{tab:opencloseRatios}): the effect of training corpus complexity is consistent. Disregarding experiments, SRNN and GRU performance improved in all measures when trained on the Low LRD corpus. For LSTMs, it improved every measure but accuracy. When distinguishing between both experiments, models trained on the Low LRD corpus performed the best on average. Furthermore, High Accuracy Models showed the lowest deviation from the ideal $1$ to $1$ open/closed misclassification ratio if they were trained on the Low LRD corpus. Finally, Low LRD trained models constitute the majority class of High Accuracy Models. On the other hand, models trained on the High LRD corpus on average underperformed compared to the Base models, regardless of model architecture and experiment.

Overall, there is strong evidence for the Low LRD training corpus leading to the most consistently good results, while the High LRD corpus tends to worsen models. This is surprising: considering how the High LRD corpus is constructed, a sizeable portion of training data is similar to the experiment data, featuring at least one long-range dependency spanning the whole word. Frequently encountering this pattern in training as well as having the model encounter more complex structures seemed likely to boost model performance. Instead, the opposite is true: learning from a less complex corpus enhanced robustness and the ability to generalize, while complex training data inhibited these processes.

These results imply not only that RNNs generalize from limited data to longer, more complex examples, they also do so by extracting generative rules from an underlying structure. This process is facilitated by giving the RNNs simple examples: once the connection is made that an open bracket must eventually be closed by its corresponding closing bracket, but not before more deeply nested pairs have been closed, the RNNs do not have to explicitly learn that the principle holds true at any nesting depth and at any character distance. Conversely, training RNNs mostly on complex words leads to an \textit{overspecification} of the learned rules: the model rarely encounters the underlying principle of $D_{2}$ in a simple form and might assume that it only holds true for specific nesting depths or distances, leading to a needlessly - and inaccurately - complex rule set. This interpretation is similar to what \cite{Zeng1993} have found when experimenting with incrementally increasing the length of strings in training: Analog RNNs have been shown to learn 'soft' solutions that are then incrementally hardened as more restrictions are necessary.