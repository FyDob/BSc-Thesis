\section{Discussion}\label{ch:discussion}
%The results presented in Chapter \ref{ch:results} show all best-performing models to score an accuracy well above random chance, indicating a successful training process. Additionally, all successful models\footnote{As defined by the criteria in Chapter \ref{ch:results}.} contain a low number of hidden units, ranging from $2$ to $8$ - no model with a higher number of hidden units has outperformed them. This shows the presented problem requiring a low minimum complexity within the models.
%
%Scoring accuracy by character index for both experiments yields two curves with markedly different development over time. Accuracy in Experiment $2$ decreases far more rapidly than in Experiment $1$, showing unseen nesting depths to constitute a far more difficult problem than resolving long-range dependencies. This hypothesis is strengthened by the fact that the same SRNN model scores an accuracy of $51.62\%$ on predicting the closing bracket of a $36$ characters long LRD, but achieves only $34.36\%$ on closing the final bracket in Experiment 2 - which constitutes a merely $28$ characters long LRD.
%
%Furthermore, the accuracy curves develop in characteristic ways. As discussed in Section \ref{resultsLRD}, $4$ distinct phases can be observed in Experiment $1$. These can be explained by the construction of the test corpus: In phase $1$, the model predicts based on an entirely unseen prefix. By character index $10$, it has reached the halfway point of the longest observed words in the training set. As the model attempts to resolve any remaining open brackets until character index $20$ in phase $2$, accuracy continues to decrease. After the model passes the maximum training word length threshold, accuracy increases in phase $3$. This could be due to the fact that character indices $19-37$ can contain a word that was encountered during training - however, accuracy decreases again after character index $28$ due to the model struggling with resolving the long-range dependency.
%
%For Experiment $2$, only $3$ phases can be observed. These phases again match the way words in the test corpus are constructed: Character indices $5$ to $25$ contain words of length $20$. During these indices, model accuracy behaves similarly to characters $1$ to $20$ in Experiment $1$. During the final $5$ characters - which contain the matching closing brackets for the open bracket suffix - accuracy plummets as the model struggles with the unseen nesting depth.
%
%As this behaviour pertains to any best-performing model, regardless of architecture and training corpus, it is strongly implied that neither model complexity, nor architecture, nor training corpus manipulation can overcome these core difficulties.
%
%While Experiments $1$ and $2$ are geared towards assessing generalizability solely on accuracy, that measure alone does not tell the full story. While the high accuracy results for models trained on the High LRD corpus imply a better performance, the confusion matrices shed another light on the result: It seems that these models exclusively predict closing brackets.\footnote{It bears repeating that the accuracy measure described in Section \ref{evaluation} was exclusively used for scoring the experiments. All models were trained with reducing loss on categorical cross-entropy for \textit{all} characters of the alphabet.} As such, all models trained on the High LRD corpus fail at learning $D_{2}$.
%
%Indeed, by taking the confusion matrices in account, it becomes clear that the models trained on the Baseline corpus or the Low LRD corpus exhibit a behaviour in line with what would be expected of a model that applies the rules underlying $D_{2}$. Not only do they never predict the wrong closing bracket instead of the correct one, all incorrect predictions are evenly split on both opening brackets. A notable exception to this are the GRU models: Only $2$ out of $6$ best-performing GRU models manage to display this understanding - both of them trained on the Low LRD corpus.
%
%This result is somewhat counter-intuitive. Both experiments demand a performance words with both a higher maximum bracket distance and a higher maximum nesting depth than is seen in any training corpus. However, providing the models with more examples of words containing a high nesting depth and a high maximum bracket distance should bolster performance on these tasks. While this approach significantly increased accuracy, it has done so at the expense of learning the rules underlying the data. Indeed, these results suggest that eschewing such comparatively difficult tasks from training data in favour of shorter, simpler words seems to greatly aid in uncovering hidden structure.
%
%The fact that models trained on either the Baseline corpus or the Low LRD corpus yield the exact same results raises the question how much more simplistic a corpus these models could be trained with while yielding similar results.
%
%Finally, comparing the confusion matrices for Experiment $1$ and $2$ yields one more insight. During Experiment $2$, the predictions in place of a closing bracket are split $1$:$1$:$1$ between both opening brackets and the correct closing bracket, while it is an approximately $1$:$1$:$2.15$ split during Experiment $1$. These results mean that the models predict more closing brackets as the word grows longer in Experiment $1$, while no such behaviour is present in Experiment $2$. The model is attempting to resolve the remaining open bracket at character index $0$, whereas such an attempt is not made in Experiment $2$, despite an LRD between the first and the final character still being present. While the models demonstrate a correct understanding of $D_{2}$ in the confusion matrices, the high nesting depth in Experiment $2$ renders the models unable to recognize and resolve the LRDs at hand. This behaviour even applies to the architectures that seek to model memory capacity.
%
%In conclusion, the results of these experiments imply several things. 
%First, GRUs, despite often being used interchangeably with LSTMs, perform significantly worse at acting according to the underlying grammar of the dataset. While they are capable of scoring an outstandingly high accuracy on both tasks, they consistently do so at the expense of never predicting opening brackets.
%
%Second, most trained models display a near perfect understanding of valid operations in $D_{2}$. While the models easily generalize to previously unseen lengths, they fail at processing high nesting depths, and thus, at learning recursion. Failing to process high nesting depths may also explain why models consistently fail to generalize when they were trained on the High LRD corpus - it also contains the highest nesting depth. If processing recursion was too difficult for a model, it was unable to extract the underlying rules of $D_{2}$ from the training data.
%
%Finally, training data construction has a significant influence on model performance - even more so than the architecture of the model. Indeed, SRNNs, LSTMs and GRUs all perform the same when trained on the Low LRD corpus. Considering that LSTMs and GRUs are uniquely equipped with mechanisms to remember or forget information appropriately, the high performance of SRNNs is surprising. It appears that memory capability is less important than rule extraction, which in turn seems to be facilitated by the appropriate choice of training corpus. The best approach appears to be to use a corpus with simple words, primarily consisting of shorter, concatenated sub-words.