@article{Cleeremans1989,
author = {Cleeremans, Axel and Servan-Schreiber, David and Mcclelland, James},
year = {1989},
month = {09},
pages = {372-381},
title = {{F}inite {S}tate {A}utomata and {S}imple {R}ecurrent {N}etworks},
volume = {1},
journal = {Neural Computation - {NECO}},
doi = {10.1162/neco.1989.1.3.372}
}

@article{Deleu2016,
  author    = {Tristan Deleu and
               Joseph Dureau},
  title     = {{L}earning {O}perations on a {S}tack with {N}eural {T}uring {M}achines},
  journal   = {CoRR},
  volume    = {abs/1612.00827},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00827},
  archivePrefix = {arXiv},
  eprint    = {1612.00827},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DeleuD16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Suzgun2019,
  author    = {Mirac Suzgun and
               Sebastian Gehrmann and
               Yonatan Belinkov and
               Stuart M. Shieber},
  title     = {{LSTM} {N}etworks {C}an {P}erform {D}ynamic {C}ounting},
  journal   = {CoRR},
  volume    = {abs/1906.03648},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.03648},
  archivePrefix = {arXiv},
  eprint    = {1906.03648},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-03648},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Skachkova2018,
	title = {Closing {B}rackets with {R}ecurrent {N}eural {N}etworks},
    author = "Skachkova, Natalia  and
      Trost, Thomas  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5425",
    doi = "10.18653/v1/W18-5425",
    pages = "232--239",
    abstract = "Many natural and formal languages contain words or symbols that require a matching counterpart for making an expression well-formed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such rules is important for language modeling. Currently, recurrent neural networks (RNNs) are extensively used for this task. We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of brackets. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the theoretical baseline in most cases.",
}

@article{Karpathy2015,
  author    = {Andrej Karpathy and
               Justin Johnson and
               Fei{-}Fei Li},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02078},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02078},
  archivePrefix = {arXiv},
  eprint    = {1506.02078},
  timestamp = {Sat, 19 Oct 2019 16:30:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KarpathyJL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bernardy2018,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  journal = {{LiLT} (Linguistic Issues in Language Technology)},
  volume = "16",
  number = "1",
  author={Jean-Philippe Bernardy},
  year={2018}
}

@article{Siegelmann1995,
title = "On the Computational Power of Neural Nets",
journal = "Journal of Computer and System Sciences",
volume = "50",
number = "1",
pages = "132 - 150",
year = "1995",
issn = "0022-0000",
doi = "https://doi.org/10.1006/jcss.1995.1013",
url = "http://www.sciencedirect.com/science/article/pii/S0022000085710136",
author = "H.T. Siegelmann and E.D. Sontag"
}

@article{Sennhauser2018,
  author    = {Luzi Sennhauser and
               Robert C. Berwick},
  title     = {Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  journal   = {CoRR},
  volume    = {abs/1811.02611},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.02611},
  archivePrefix = {arXiv},
  eprint    = {1811.02611},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-02611},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Rodriguez1998,
 author = {Rodriguez, Paul and Wiles, Janet},
 title = {Recurrent {N}eural {N}etworks {C}an {L}earn to {I}mplement {S}ymbol-sensitive {C}ounting},
 booktitle = {Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems 10},
 series = {NIPS '97},
 year = {1998},
 isbn = {0-262-10076-2},
 location = {Denver, Colorado, USA},
 pages = {87--93},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=302528.302552},
 acmid = {302552},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@InProceedings{Li2018,
  title = 	 {Nonlinear {W}eighted {F}inite {A}utomata},
  author = 	 {Tianyu Li and Guillaume Rabusseau and Doina Precup},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {679--688},
  year = 	 {2018},
  editor = 	 {Amos Storkey and Fernando Perez-Cruz},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Playa Blanca, Lanzarote, Canary Islands},
  month = 	 {09--11 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/li18a/li18a.pdf},
  url = 	 {http://proceedings.mlr.press/v84/li18a.html},
  abstract = 	 {Weighted finite automata (WFA) can expressively model functions defined over strings but are inherently linear models.Given the recent successes of non-linear models in machine learning, it is natural to wonder whether extending WFA to the non-linearsetting would be beneficial.In this paper, we propose a novel model of neural network based nonlinear WFA model (NL-WFA) along with a learning algorithm. Our learning algorithm is inspired by the spectral learning algorithm for WFA and relies on a non-linear decomposition of the so-called Hankel matrix, by means of an auto-encoder network. The expressive power of NL-WFA and the proposed learning algorithm are assessed on both synthetic and real world data, showing that NL-WFA can infer complex grammatical structures from data.}
}

@article{Joulin2015,
  author    = {Armand Joulin and
               Tomas Mikolov},
  title     = {{I}nferring {A}lgorithmic {P}atterns with {S}tack-{A}ugmented {R}ecurrent {N}ets},
  journal   = {CoRR},
  volume    = {abs/1503.01007},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.01007},
  archivePrefix = {arXiv},
  eprint    = {1503.01007},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JoulinM15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yu2019,
    title = "Learning the {D}yck {L}anguage with {A}ttention-based {S}eq2{S}eq {M}odels",
    author = "Yu, Xiang  and
      Vu, Ngoc Thang  and
      Kuhn, Jonas",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4815",
    doi = "10.18653/v1/W19-4815",
    pages = "138--146",
    abstract = "The generalized Dyck language has been used to analyze the ability of Recurrent Neural Networks (RNNs) to learn context-free grammars (CFGs). Recent studies draw conflicting conclusions on their performance, especially regarding the generalizability of the models with respect to the depth of recursion. In this paper, we revisit several common models and experimental settings, discuss the potential problems of the tasks and analyses. Furthermore, we explore the use of attention mechanisms within the seq2seq framework to learn the Dyck language, which could compensate for the limited encoding ability of RNNs. Our findings reveal that attention mechanisms still cannot truly generalize over the recursion depth, although they perform much better than other models on the closing bracket tagging task. Moreover, this also suggests that this commonly used task is not sufficient to test a model{'}s understanding of CFGs.",
}

@ARTICLE{GersSchmidhuber2001,
author={F. A. {Gers} and E. {Schmidhuber}},
journal={{IEEE} Transactions on Neural Networks},
title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
year={2001},
volume={12},
number={6},
pages={1333-1340},
keywords={recurrent neural nets;learning (artificial intelligence);context-free languages;context-sensitive languages;regular languages;long short-term memory;recurrent neural networks;context-free language;context-sensitive language;Recurrent neural networks;Hidden Markov models;Delay effects;Backpropagation algorithms;Resonance light scattering;Learning automata;Neural networks;State-space methods;Bridges;Computational complexity},
doi={10.1109/72.963769},
ISSN={},
month={Nov},}

@Article{Zeng1994,
  author  = {Zheng Zeng and Rodney M. Goodman and Padhraic Smyth},
  title   = {Discrete recurrent neural networks for grammatical inference},
  journal = {{IEEE} transactions on neural networks},
  year    = {1994},
  volume  = {5 2},
  pages   = {320-30},
  file    = {:Zeng Goodman and Smyth 1994.pdf:PDF},
}

@book{JurafskyMartin2009,
 author = {Jurafsky, Daniel and Martin, James H.},
 title = {Speech and Language Processing (2nd Edition)},
 booktitle = {Speech and Language Processing (2nd Edition)},
 year = {2009},
 isbn = {0131873210},
 publisher = {Prentice-Hall, Inc.},
 address = {Upper Saddle River, NJ, USA},
}

@article{Chomsky1959,
title = "On certain formal properties of grammars",
journal = "Information and Control",
volume = "2",
number = "2",
pages = "137 - 167",
year = "1959",
issn = "0019-9958",
doi = "https://doi.org/10.1016/S0019-9958(59)90362-6",
url = "http://www.sciencedirect.com/science/article/pii/S0019995859903626",
author = "Noam Chomsky",
abstract = "A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail."
}

@ARTICLE{Chomsky1956,
author={N. {Chomsky}},
journal={IRE Transactions on Information Theory},
title={Three models for the description of language},
year={1956},
volume={2},
number={3},
pages={113-124},
keywords={Languages;Markov processes;Natural languages;Testing;Laboratories;Markov processes;Impedance matching;Kernel;Research and development},
doi={10.1109/TIT.1956.1056813},
ISSN={2168-2712},
month={Sep.},}

@inproceedings{Branco2018,
    title = "Computational Complexity of Natural Languages: A Reasoned Overview",
    author = "Branco, Ant{\'o}nio",
    booktitle = "Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing",
    month = aug,
    year = "2018",
    address = "Santa Fe, New-Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4602",
    pages = "10--19",
    abstract = "There has been an upsurge of research interest in natural language complexity. As this interest will benefit from being informed by established contributions in this area, this paper presents a reasoned overview of central results concerning the computational complexity of natural language parsing. This overview also seeks to help to understand why, contrary to recent and widespread assumptions, it is by no means sufficient that an agent handles sequences of items under a pattern $a^n b^n$ or under a pattern $a^n b^m c^n d^m$ to ascertain ipso facto that this is the result of at least an underlying context-free grammar or an underlying context-sensitive grammar, respectively. In addition, it seeks to help to understand why it is also not sufficient that an agent handles sequences of items under a pattern $a^n b^n$ for it to be deemed as having a cognitive capacity of higher computational complexity.",
}

@article{Hagege1976,
 ISSN = {00243892, 15309150},
 URL = {http://www.jstor.org/stable/4177917},
 author = {Claude Hag{\`e}ge},
 journal = {Linguistic Inquiry},
 number = {1},
 pages = {198--201},
 publisher = {The MIT Press},
 title = {{R}elative {C}lause, {C}enter-{E}mbedding, and {C}omprehensibility},
 volume = {7},
 year = {1976}
}

@article{Karlsson2007,
 ISSN = {00222267, 14697742},
 URL = {http://www.jstor.org/stable/40057996},
 abstract = {A common view in theoretical syntax and computational linguistics holds that there are no grammatical restrictions on multiple center-embedding of clauses. Syntax would thus be characterized by unbounded recursion. An analysis of 119 genuine multiple clausal center-embeddings from seven 'Standard Average European' languages (English, Finnish, French, German, Latin, Swedish, Danish) uncovers usage-based regularities, constraints, that run counter to these and several other widely held views, such as that any type of multiple self-embedding (of the same clause type) would be possible, or that self-embedding would be more complex than multiple center-embedding of different clause types. The maximal degree of center-embedding in written language is three. In spoken language, multiple center- embedding is practically absent. Typical center-embeddings of any degree involve relative clauses specifying the referent of the subject NP of the superordinate clause. Only postmodifying clauses, especially relative clauses and that-clauses acting as noun complements, allow central self-embedding. Double relativization of objects ( The rat the cat the dog chased killed ate the malt) does not occur. These corpus-based ' soft constraints ' suggest that full-blown recursion creating multiple clausal center- embedding is not a central design feature of language in use. Multiple center- embedding emerged with the advent of written language, with Aristotle, Cicero, and Livy in the Greek and Latin stylistic tradition of 'periodic' sentence composition.},
 author = {Fred Karlsson},
 journal = {Journal of Linguistics},
 number = {2},
 pages = {365--392},
 publisher = {Cambridge University Press},
 title = {{C}onstraints on {M}ultiple {C}enter-{E}mbedding of {C}lauses},
 volume = {43},
 year = {2007}
}

@article{Hamilton1971,
title = "Comprehensibility and subject-verb relations in complex sentences",
journal = "Journal of Verbal Learning and Verbal Behavior",
volume = "10",
number = "2",
pages = "163 - 170",
year = "1971",
issn = "0022-5371",
doi = "https://doi.org/10.1016/S0022-5371(71)80008-7",
url = "http://www.sciencedirect.com/science/article/pii/S0022537171800087",
author = "Helen W. Hamilton and James Deese",
abstract = "This study investigates the influence of (a) the surface form of complex sentences and (b) the appropriateness of the nouns which are subjects of the various clauses in these sentences for their respective verbs upon the comprehensibility of sentences. Base sentences cast as right-branching sentences are much more comprehensible than the same sentences cast in a center-embedded form. Furthermore, an intermediate case is intermediate in comprehensibility, and both the center-embedded and intermediate forms are sensitive to added clauses, while the right-branching form is not. Appropriateness of subject-noun and verb is a poor predictor of comprehensibility. These and earlier results lead to the conclusion that grammatical structure and not semantic relations is the major determiner of comprehensibility."
}

@article{Frank2016,
author = {Frank, Stefan L. and Trompenaars, Thijs and Vasishth, Shravan},
title = {Cross-{L}inguistic {D}ifferences in {P}rocessing {D}ouble-{E}mbedded {R}elative {C}lauses: {W}orking-{M}emory {C}onstraints or {L}anguage {S}tatistics?},
journal = {Cognitive Science},
volume = {40},
number = {3},
pages = {554-578},
keywords = {Bilingualism, Cross-linguistic differences, Sentence comprehension, Relative clauses, Centre embedding, Grammaticality illusion, Self-paced reading, Recurrent neural network model},
doi = {10.1111/cogs.12247},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12247},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12247},
abstract = {Abstract An English double-embedded relative clause from which the middle verb is omitted can often be processed more easily than its grammatical counterpart, a phenomenon known as the grammaticality illusion. This effect has been found to be reversed in German, suggesting that the illusion is language specific rather than a consequence of universal working memory constraints. We present results from three self-paced reading experiments which show that Dutch native speakers also do not show the grammaticality illusion in Dutch, whereas both German and Dutch native speakers do show the illusion when reading English sentences. These findings provide evidence against working memory constraints as an explanation for the observed effect in English. We propose an alternative account based on the statistical patterns of the languages involved. In support of this alternative, a single recurrent neural network model that is trained on both Dutch and English sentences is shown to predict the cross-linguistic difference in the grammaticality effect.},
year = {2016}
}

@Incollection{Shieber1987,
author="Shieber, Stuart M.",
title="{E}vidence {A}gainst the {C}ontext-{F}reeness of {N}atural {L}anguage",
bookTitle="The Formal Complexity of Natural Language",
year="1987",
publisher="Springer Netherlands",
address="Dordrecht",
pages="320--334",
abstract="In searching for universal constraints on the class of natural languages, linguists have investigated a number of formal properties, including that of context-freeness. Soon after Chomsky's categorization of languages into his well-known hierarchy (Chomsky, 1963), the common conception of the context-free class of languages as a tool for describing natural languages was that it was too restrictive a class --- interpreted strongly (as a way of characterizing structure sets) and even weakly (as a way of characterizing string sets).",
isbn="978-94-009-3401-6",
doi="10.1007/978-94-009-3401-6_12",
url="https://doi.org/10.1007/978-94-009-3401-6_12"
}

@article{Bach1986,
author = {Bach, Emmon and Brown, Colin and Marslen-Wilson, William},
year = {1986},
month = {10},
pages = {249-262},
title = {Crossed and nested dependencies in {G}erman and {D}utch: A psycholinguistic study},
volume = {1},
journal = {Language and Cognitive Processes},
doi = {10.1080/01690968608404677}
}

@Incollection{Autebert1997,
author="Autebert, Jean-Michel
and Berstel, Jean
and Boasson, Luc",
title="{C}ontext-{F}ree {L}anguages and {P}ushdown {A}utomata",
pages="111--174",
abstract="This chapter is devoted to context-free languages. Context-free languages and grammars were designed initially to formalize grammatical properties of natural languages [9]. They subsequently appeared to be well adapted to the formal description of the syntax of programming languages. This led to a considerable development of the theory.",
crossref = {HandbookOfFormalLanguages}
}

@book{HandbookOfFormalLanguages,
title="Handbook of Formal Languages: Volume 1 Word, Language, Grammar",
booktitle="Handbook of Formal Languages: Volume 1 Word, Language, Grammar",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
isbn="978-3-642-59136-5",
doi="10.1007/978-3-642-59136-5_3",
url="https://doi.org/10.1007/978-3-642-59136-5_3"
}

@incollection{Chomsky1963,
title = "The {A}lgebraic {T}heory of {C}ontext-{F}ree {L}anguages*",
series = "Studies in Logic and the Foundations of Mathematics",
publisher = "Elsevier",
volume = "35",
pages = "118 - 161",
year = "1963",
booktitle = "Computer Programming and Formal Systems",
issn = "0049-237X",
doi = "https://doi.org/10.1016/S0049-237X(08)72023-8",
url = "http://www.sciencedirect.com/science/article/pii/S0049237X08720238",
author = "N. Chomsky and M.P. Schützenberger",
abstract = "Publisher Summary
This chapter discusses the several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. By a language it simply mean a set of strings in some finite set V of symbols called the vocabulary of the language. By a grammar a set of rules that give a recursive enumeration of the strings belonging to the language. It can be said that the grammar generates these strings. The chapter discusses the aspect of the structural description of a sentence, namely, its subdivision into phrases belonging to various categories. A major concern of the general theory of natural languages is to define the class of possible strings; the class of possible grammars; the class of possible structural descriptions; a procedure for assigning structural descriptions to sentences, given a grammar; and to do all of this in such a way that the structural description assigned to a sentence by the grammar of a natural language will provide the basis for explaining how a speaker of this language would understand this sentence."
}

@article{Elman1990,
author = {Elman, Jeffrey L.},
title = {{F}inding {S}tructure in {T}ime},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
doi = {10.1207/s15516709cog1402\_1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
year = {1990}
}

@ARTICLE{Bengio1994,
author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},
journal={{IEEE} Transactions on Neural Networks},
title={Learning long-term dependencies with gradient descent is difficult},
year={1994},
volume={5},
number={2},
pages={157-166},
keywords={recurrent neural nets;learning (artificial intelligence);numerical analysis;long-term dependencies;gradient descent;recognition;production problems;prediction problems;recurrent neural network training;temporal contingencies;input/output sequence mapping;efficient learning;Recurrent neural networks;Production;Delay effects;Intelligent networks;Neural networks;Discrete transforms;Computer networks;Cost function;Neurofeedback;Displays},
doi={10.1109/72.279181},
ISSN={1941-0093},
month={March},}

@article{Hochreiter1998,
author = {Hochreiter, Sepp},
year = {1998},
month = {04},
pages = {107-116},
title = {The {V}anishing {G}radient {P}roblem {D}uring {L}earning {R}ecurrent {N}eural {N}ets and {P}roblem {S}olutions},
volume = {6},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
doi = {10.1142/S0218488598000094}
}

@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long {S}hort-term {M}emory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@incollection{Williams1998,
author = {Williams, Ronald J. and Zipser, David},
title = {Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity},
year = {1995},
isbn = {0805812598},
publisher = {L. Erlbaum Associates Inc.},
address = {USA},
booktitle = {Backpropagation: Theory, Architectures, and Applications},
pages = {433–486},
numpages = {54}
}

@inproceedings{Cho2014,
    title = "{L}earning {P}hrase {R}epresentations using {RNN} {E}ncoder{--}{D}ecoder for {S}tatistical {M}achine {T}ranslation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{Kingma2014,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A {M}ethod for {S}tochastic {O}ptimization},
journal = {International Conference on Learning Representations}
}

@article{Koutnik2014,
author = {Koutn{í}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{ü}rgen},
year = {2014},
month = {02},
pages = {},
title = {A clockwork {RNN}},
volume = {5},
journal = {31st International Conference on Machine Learning, ICML 2014}
}

@book{Sipser2013,
  added-at = {2014-03-03T20:31:26.000+0100},
  address = {Boston, MA},
  author = {Sipser, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2a275d239d3a005a2a0825e49ce8dced5/ytyoun},
  edition = {Third},
  interhash = {ba5fd05e9f15a677c2c9e619c57de9a7},
  intrahash = {a275d239d3a005a2a0825e49ce8dced5},
  isbn = {113318779X},
  keywords = {automata complexity computation hamiltonian np-hardness sipser textbook},
  publisher = {Course Technology},
  refid = {814441519},
  timestamp = {2016-12-04T08:23:19.000+0100},
  title = {Introduction to the Theory of Computation},
  booktitle = {Introduction to the Theory of Computation},
  year = 2013
}

@book{Hopcroft2006,
author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
title = {Introduction to Automata Theory, Languages, and Computation (3rd Edition)},
booktitle = {Introduction to Automata Theory, Languages, and Computation (3rd Edition)},
year = {2006},
isbn = {0321455363},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@article{Fitch2012,
author = {Fitch, W. Tecumseh  and Friederici, Angela D.  and Hagoort, Peter },
title = {Pattern perception and computational complexity: introduction to the special issue},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
volume = {367},
number = {1598},
pages = {1925-1932},
year = {2012},
doi = {10.1098/rstb.2012.0099},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2012.0099},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2012.0099},
abstract = {Research on pattern perception and rule learning, grounded in formal language theory (FLT) and using artificial grammar learning paradigms, has exploded in the last decade. This approach marries empirical research conducted by neuroscientists, psychologists and ethologists with the theory of computation and FLT, developed by mathematicians, linguists and computer scientists over the last century. Of particular current interest are comparative extensions of this work to non-human animals, and neuroscientific investigations using brain imaging techniques. We provide a short introduction to the history of these fields, and to some of the dominant hypotheses, to help contextualize these ongoing research programmes, and finally briefly introduce the papers in the current issue.}
}

@article{Petersson2012,
author = {Petersson, Karl Magnus and Hagoort, Peter},
year = {2012},
month = {07},
pages = {1971-83},
title = {The neurobiology of syntax: Beyond string sets},
volume = {367},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
doi = {10.1098/rstb.2012.0101}
}

@book{Newmeyer2014,
  title={Measuring Grammatical Complexity},
  booktitle={Measuring Grammatical Complexity},
  author={Newmeyer, F.J. and Preston, L.B.},
  isbn={9780199685301},
  lccn={2014932091},
  series={Oxford linguistics},
  url={https://books.google.de/books?id=jjADBQAAQBAJ},
  year={2014},
  publisher={Oxford University Press}
}

@article{Zeng1993,
  title={Learning {F}inite {S}tate {M}achines {W}ith {S}elf-{C}lustering {R}ecurrent {N}etworks},
  author={Zheng Zeng and Rodney M. Goodman and Padhraic Smyth},
  journal={Neural Computation},
  year={1993},
  volume={5},
  pages={976-990}
}

@article{scikit-learn2011,
 title={Scikit-learn: {M}achine {L}earning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@book{Oliphant2006, 
  title={A guide to NumPy}, 
  booktitle={A guide to NumPy}, 
  author={Oliphant, Travis E}, 
  volume={1}, 
  year={2006}, 
  publisher={Trelgol Publishing USA} 
}

@article{Mckinney2011,
    title={pandas: a foundational {P}ython library for data analysis and statistics},
    author={Wes McKinney},
    journal={Python for High Performance and Scientific Computing},
    volume={14},
    year={2011}
}